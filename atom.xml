<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[刘邦的博客]]></title>
  <link href="http://iliubang.github.io/atom.xml" rel="self"/>
  <link href="http://iliubang.github.io/"/>
  <updated>2016-10-27T23:46:26+08:00</updated>
  <id>http://iliubang.github.io/</id>
  <author>
    <name><![CDATA[]]></name>
    
  </author>
  <generator uri="http://www.mweb.im/">MWeb</generator>
  
  <entry>
    <title type="html"><![CDATA[基于C语言的编程语言开发 -- yacc/lex初步]]></title>
    <link href="http://iliubang.github.io/14775733948936.html"/>
    <updated>2016-10-27T21:03:14+08:00</updated>
    <id>http://iliubang.github.io/14775733948936.html</id>
    <content type="html"><![CDATA[
<h2 id="toc_0">Preface</h2>

<p>当今世道，各种高级语言百花齐放。然而会有人发出这样的疑问--计算机真的能够识别这么多语言吗？稍微有点文化的人都知道，这显然是不可能滴！在计算机的世界里，他们能够直接识别的只有机器语言。然而，由于机器语言对人类不够友好，所以人们才发明了汇编，c，Java...许许多多的人类易读的编程语言，所以我个人对编程语言的理解一直是其实他们就是机器语言的语法糖，而编程语言的创造过程，就是定义一种合理的，没有二义性的语法规则，然后就是通过直接或间接的方式实现该语法到机器语言的转换过程。既然是这样的话，那么我们就很容易想到，计算机语言是一个自我完善的过程：首先我们定了一种非常简单的x1(这里只是用来举例说明，有没有x语言有待考证)语言，然后用机器语言实现了这个非常简单的x1语言的编译器，创造了x1语言，实现了非常简单的新特性，然后我们再用x1语言(相对于机器语言较高级)实现了另一些新的特性的x2语言的编译器，创造了x2语言，...，如此下去，人们创造了汇编语言，从而创造了c语言，接着创造了世界上最好的语言PHP(不知道是不是真的，反正大家都习惯这么说)。<br/>
在各种高级语言越来越强大的今天，我们可能很难再会去接触最原始的东西，高度封装确实提高了生产力，降低了学习成本，但是也使得现代程序员将太多精力花在了各种说明书上，而不清楚其本质。<br/>
毕业一年多，工作了一年多，对于计算机编程有了自己的看法，不再像在大学的时候认识的那样肤浅，反而觉得大学中学习的知识才是真正的干货，不禁感叹曾经浪费掉了大好光阴。好在陶渊明有词云：“悟已往之不谏，知来者之可追”。<br/>
闲暇之余，扒开PHP(这里之所以是PHP并不因为他是世界上最好的语言，只是因为我目前从事的是PHP开发的工作而已)源码，了解了其内部构造和实现原理，百看不一练。今天就初步学习yacc/lex了，记录在我的博客中，以便以后翻阅巩固。</p>

<h2 id="toc_1">过程简述</h2>

<p>一般来说编程语言的解释执行过程如下：</p>

<p><strong>ONE</strong>. 词法分析<br/>
将源代码拆分成若干Token的过程</p>

<p><strong>TWO</strong>.语法分析<br/>
将Token构建成Syntax Tree的过程</p>

<p><strong>THREE</strong>.生成执行码<br/>
生成可执行文件</p>

<h2 id="toc_2">yacc（Yet Another Compiler Compiler）</h2>

<p>下面是wikipedia中对yacc的描述</p>

<blockquote>
<p>Yacc is a computer program for the Unix operating system. It is a Look Ahead Left-to-Right (LALR) parser generator, generating a parser, the part of a compiler that tries to make syntactic sense of the source code, specifically a LALR parser, based on an analytic grammar written in a notation similar to Backus–Naur Form (BNF). Yacc itself used to be available as the default parser generator on most Unix systems, though it has since been supplanted as the default by more recent, largely compatible, programs.</p>
</blockquote>

<p>其安装非常简单</p>

<pre><code class="language-bash">sudo apt-get install bison
</code></pre>

<h2 id="toc_3">lex/flex</h2>

<p>lex 是一个生成词法分析器的工具。Lex读进一个代表词法分析器规则的输入字符串流，然后输出以C语言实做的词法分析器源代码。传统上，lex属于商业软件，但是有些根据原本AT&amp;T代码这些版本的Lex可以以公开源代码的形式获得，并被视为某些系统的一部分，例如说OpenSolaris和贝尔实验室九号项目。另一个有名的Lex公开源代码版本是flex，代表&quot;快速的词法分析器&quot;（fast lexical analyzer）</p>

<p>在linux下安装</p>

<pre><code class="language-bash">sudo apt-get install flex
</code></pre>

<h2 id="toc_4">practice</h2>

<ul>
<li>实现一个简单的计算程序</li>
</ul>

<p>首先定义lex规则，其扩展名为<code>.l</code>，在lex中可以很容易读懂其定义的规则，因为他用到的是正则表达式。</p>

<pre><code class="language-c">%{
/*
 |------------------------------------------------------------------
 | linger test
 |------------------------------------------------------------------
 | @author    : liubang
 | @date      : 16/10/27 下午8:28
 | @copyright : (c) iliubang.cn
 | @license   : MIT (http://opensource.org/licenses/MIT)
 |------------------------------------------------------------------
 */

#include &lt;stdio.h&gt;
#include &quot;y.tab.h&quot;

int yywrap(void)
{
    return 1;
}
%}

%%

&quot;+&quot;     return ADD;
&quot;-&quot;     return SUB;
&quot;*&quot;     return MUL;
&quot;/&quot;     return DIV;
&quot;\n&quot;        return CR;

([1-9][0-9]*)|0|([0-9]+\.[0-9]+) {
    double d;
    sscanf(yytext, &quot;%lf&quot;, &amp;d);
    yylval.double_value = d;
    return DOUBLE_LITERAL;
}

[ \t] ;
. {
    fprintf(stderr, &quot;lexical error.\n&quot;);
    exit(1);
}
%%

</code></pre>

<p>可以看到以上的代码主要包含两部分，<code>%{</code> <code>%}</code>包含的部分和<code>%%</code> <code>%%</code>包含的部分。前一部分叫<strong>定义区块</strong>, 后者是<strong>规则区块</strong>，定义区块内的代码将会被原样输出，在定义区块中<code>#include &quot;y.tab.h&quot;</code>将会在yacc编译其规则文件后自动生成，<code>ADD</code> <code>SUB</code> <code>MUL</code> <code>DIV</code> <code>CR</code> <code>DOUBLE_LITERAL</code>等都是在y.tab.h中定义的macro。<br/>
在定义区块中，有一个名为<code>yywrap</code>的function，其作用是自动link lex的库文件。<br/>
至于规则区块，学过正则表达式的人一看就会明白，其作用就是使用正则表达式来描述Token。规则区块的定义为：正则表达式，后边跟上C代码，这些代码用<code>{}</code>括起来，读入的字符流满足了正则，则执行其后的代码，匹配到的原字符被保存在<code>yytext</code>这个全局变量中。</p>

<p>接着来编写yacc的规则，其扩展名为<code>.y</code>。</p>

<pre><code class="language-c">%{
/*
 |------------------------------------------------------------------
 | linger test
 |------------------------------------------------------------------
 | @author    : liubang
 | @date      : 16/10/27 下午8:43
 | @copyright : (c) iliubang.cn
 | @license   : MIT (http://opensource.org/licenses/MIT)
 |------------------------------------------------------------------
 */

#include &lt;stdio.h&gt;
#include &lt;stdlib.h&gt;
#define YYDEBUG 1

%}

%union {
    int int_value;
    double  double_value;
}

%token &lt;double_value&gt;       DOUBLE_LITERAL
%token ADD SUB MUL DIV CR
%type &lt;double_value&gt; expression term primary_expression

%%

line_list
    : line
    | line_list line
;
line
    : expression CR
    {
        printf(&quot;&gt;&gt;%lf\n&quot;, $1);
    }
;
expression
    : term
    | expression ADD term
    {
        $$ = $1 + $3;
    }
    | expression SUB term
    {
        $$ = $1 - $3;
    }
;
term
    : primary_expression
    | term MUL primary_expression
    {
        $$ = $1 * $3;
    }
    | term DIV primary_expression
    {
        $$ = $1 / $3;
    }
;
primary_expression
    : DOUBLE_LITERAL
;

%%

int yyerror(char const *str)
{
    extern char *yytext;
    fprintf(stderr, &quot;syntax error near %s\n&quot;, yytext);
    return 0;
}

int main(void)
{
    extern int yyparse(void);
    extern FILE *yyin;

    yyin = stdin;
    if (yyparse()) {
        fprintf(stderr, &quot;Core Dump!\n&quot;);
        exit(1);
    }
}
</code></pre>

<p>yacc规则定义跟lex相似，都用到了<code>%{%}</code> <code>%%</code>来包含代码块。<br/>
同样的是<code>%{%}</code>包裹的代码将被原样输出。<br/>
在<code>%union</code>定义中，声明了记号和非终结符的类型，其最终会被编译成一个c语言的union。这里定义了一个int 类型的int_value 和 double类型的double_value。<br/>
<code>%token</code>开头的行是Token的声明，所有用到的Token类型都在这里定义。对于<code>ADD</code> <code>SUB</code> <code>MUL</code> <code>DIV</code> <code>CR</code>等记号只需要包含其类型即可，而对于值为<code>DOUBLE_LITERAL</code>的Token，其类型被指定为<code>&lt;double_value&gt;</code>，这里的double_value正是来自于前面声明的union中的成员之一。<br/>
<code>%%</code>包裹的部分叫做规则区块，由语法规则和C语言编写的相应的行为两部分构成。在yacc中使用了类似于BNF范式来编写语法规则。由于使用了自然语言作为标记，理解上还是很容易的。下面举个简单的例子：</p>

<pre><code class="language-c">line_list                 /* 多行规则 */
    : line                  /* 单行 */
    | line_list line        /* 或者多行后跟单行 */
;
line                       /* 单行 */
    : expression CR          /* 表达式后跟换行符 */
;
expression                 /* 表达式 */
    : term                   /* 和项 */
    | expression ADD term    /* 或者表达式加上和项 */
    {                        /* 匹配后执行的action */
        $$ = $1 + $3;
    }
    | expression SUB term    /* 或者表达式减去和项 */
    {                        /* 匹配后执行的action */
        $$ = $1 - $3;
    }
;
term                        /* 和项 */
    : primary_expression      /* 一元表达式 */
    | term MUL primary_expression /* 或者和项乘以一元表达式 */
    {                          /* 匹配后执行的动作 */
        $$ = $1 * $3;
    }                          
    | term DIV primary_expression /* 或者和项除以一元表达式 */
    {                             /* 匹配后执行的动作 */
        $$ = $1 / $3;
    }
;
primary_expression          /* 一元表达式 */
    : DOUBLE_LITERAL          /* 实数字面量 */
;
</code></pre>

<p>写好了规则，那么就来编译运行</p>

<pre><code class="language-bash">yacc -dv foo.y
flex foo.l
gcc -std=c99 -Wall -g -o foo y.tab.c lex.yy.c
</code></pre>

<p>这样就生成了<code>foo</code>这个可执行文件<br/>
运行foo</p>

<pre><code class="language-bash">liubang@venux:~/workspace/c/my_lang/lex$ make run
1 + 3
&gt;&gt;4.000000
1/2 
&gt;&gt;0.500000
4 * 5
&gt;&gt;20.000000
</code></pre>

<p>当然我个人在开发c程序的时候偏向于使用make工具来编译代码，这样会很方便。下面是我的Makefile文件:</p>

<pre><code class="language-makefile">CFLAGS = -O2 -g -Wall -std=c99
EXEC = foo
OBJS =  y.tab.o \
    lex.yy.o


%.c:
    yacc -dv foo.y
    lex foo.l

%.o: %.c
     $(CC) $(CFLAGS) -o $@ -c $&lt;

$(EXEC): $(OBJS)
    $(CC) $(OBJS) -o $@

all: $(EXEC)

run: $(EXEC)
    @./$(EXEC)

clean:
    $(RM) $(OBJS) $(EXEC)
</code></pre>

<p>只需要执行<code>make run</code>命令即可运行！</p>

<h2 id="toc_5">附加</h2>

<p>可能有人会疑惑，这玩意学着有什么用，那么有兴趣的你可以下载一份PHP的源代码，在Zend(Zend引擎核心文件)目录中你不难找到<code>zend_language_scanner.l</code>,<code>zend_ini_scanner.l</code>,<code>zend_ini_parser.y</code>,<code>zend_language_parser.y</code>这几个文件，打开其内容，是不是不再那么恐惧和陌生了呢。</p>

<h2 id="toc_6">Summary</h2>

<p>本文只是初步介绍yacc/lex工具生成词法解析器和语法解析器的最基本用法，没有太多的阐述词法解析的原理和过程，所以更偏实践，再由于本人毕业一年多实在很久没写文章了，常常会提笔不知从何说起，所以写起来很慢，加之白天要工作，时间较紧，所以今天就到这里了，至于理论的阐述，需要时间来慢慢酝酿😂！</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[An Introduction to Lock-Free Programming]]></title>
    <link href="http://iliubang.github.io/14771244487868.html"/>
    <updated>2016-10-22T16:20:48+08:00</updated>
    <id>http://iliubang.github.io/14771244487868.html</id>
    <content type="html"><![CDATA[
<p><strong>[<a href="http://preshing.com/20120612/an-introduction-to-lock-free-programming/">Reference</a>]</strong></p>

<p>Lock-free programming is a challenge, not just because of the complexity of the task itself, but because of how difficult it can be to penetrate the subject in the first place.</p>

<p>I was fortunate in that my first introduction to lock-free (also known as lockless) programming was Bruce Dawson’s excellent and comprehensive white paper, Lockless Programming Considerations. And like many, I’ve had the occasion to put Bruce’s advice into practice developing and debugging lock-free code on platforms such as the Xbox 360.</p>

<p>Since then, a lot of good material has been written, ranging from abstract theory and proofs of correctness to practical examples and hardware details. I’ll leave a list of references in the footnotes. At times, the information in one source may appear orthogonal to other sources: For instance, some material assumes sequential consistency, and thus sidesteps the memory ordering issues which typically plague lock-free C/C++ code. The new C++11 atomic library standard throws another wrench into the works, challenging the way many of us express lock-free algorithms.</p>

<p>In this post, I’d like to re-introduce lock-free programming, first by defining it, then by distilling most of the information down to a few key concepts. I’ll show how those concepts relate to one another using flowcharts, then we’ll dip our toes into the details a little bit. At a minimum, any programmer who dives into lock-free programming should already understand how to write correct multithreaded code using mutexes, and other high-level synchronization objects such as semaphores and events.</p>

<h2 id="toc_0">What Is It?</h2>

<p>People often describe lock-free programming as programming without mutexes, which are also referred to as locks. That’s true, but it’s only part of the story. The generally accepted definition, based on academic literature, is a bit more broad. At its essence, lock-free is a property used to describe some code, without saying too much about how that code was actually written.</p>

<p>Basically, if some part of your program satisfies the following conditions, then that part can rightfully be considered lock-free. Conversely, if a given part of your code doesn’t satisfy these conditions, then that part is not lock-free.<br/>
<img src="media/14771244487868/its-lock-free.png" alt="its-lock-free"/></p>

<p>In this sense, the lock in lock-free does not refer directly to mutexes, but rather to the possibility of “locking up” the entire application in some way, whether it’s deadlock, livelock – or even due to hypothetical thread scheduling decisions made by your worst enemy. That last point sounds funny, but it’s key. Shared mutexes are ruled out trivially, because as soon as one thread obtains the mutex, your worst enemy could simply never schedule that thread again. Of course, real operating systems don’t work that way – we’re merely defining terms.</p>

<p>Here’s a simple example of an operation which contains no mutexes, but is still not lock-free. Initially, X = 0. As an exercise for the reader, consider how two threads could be scheduled in a way such that neither thread exits the loop.</p>

<pre><code class="language-c">while (X == 0)
{
    X = 1 - X;
}
</code></pre>

<p>Nobody expects a large application to be entirely lock-free. Typically, we identify a specific set of lock-free operations out of the whole codebase. For example, in a lock-free queue, there might be a handful of lock-free operations such as <code>push</code>, <code>pop</code>, perhaps <code>isEmpty</code>, and so on.</p>

<p>Herlihy &amp; Shavit, authors of The Art of Multiprocessor Programming, tend to express such operations as class methods, and offer the following succinct definition of lock-free (see slide 150): “In an infinite execution, infinitely often some method call finishes.” In other words, as long as the program is able to keep calling those lock-free operations, the number of completed calls keeps increasing, no matter what. It is algorithmically impossible for the system to lock up during those operations.</p>

<p>One important consequence of lock-free programming is that if you suspend a single thread, it will never prevent other threads from making progress, as a group, through their own lock-free operations. This hints at the value of lock-free programming when writing interrupt handlers and real-time systems, where certain tasks must complete within a certain time limit, no matter what state the rest of the program is in.</p>

<p>A final precision: Operations that are designed to block do not disqualify the algorithm. For example, a queue’s pop operation may intentionally block when the queue is empty. The remaining codepaths can still be considered lock-free.</p>

<h2 id="toc_1">Lock-Free Programming Techniques</h2>

<p>It turns out that when you attempt to satisfy the non-blocking condition of lock-free programming, a whole family of techniques fall out: atomic operations, memory barriers, avoiding the ABA problem, to name a few. This is where things quickly become diabolical.</p>

<p>So how do these techniques relate to one another? To illustrate, I’ve put together the following flowchart. I’ll elaborate on each one below.<br/>
<img src="media/14771244487868/techniques.png" alt="techniques"/></p>

<h2 id="toc_2">Atomic Read-Modify-Write Operations</h2>

<p>Atomic operations are ones which manipulate memory in a way that appears indivisible: No thread can observe the operation half-complete. On modern processors, lots of operations are already atomic. For example, aligned reads and writes of simple types are usually atomic.</p>

<p>Read-modify-write (RMW) operations go a step further, allowing you to perform more complex transactions atomically. They’re especially useful when a lock-free algorithm must support multiple writers, because when multiple threads attempt an RMW on the same address, they’ll effectively line up in a row and execute those operations one-at-a-time. I’ve already touched upon RMW operations in this blog, such as when implementing a lightweight mutex, a recursive mutex and a lightweight logging system.</p>

<p>Examples of RMW operations include <code>_InterlockedIncrement</code> on Win32, <code>OSAtomicAdd32</code> on iOS, and <code>std::atomic&lt;int&gt;::fetch_add</code> in C++11. Be aware that the C++11 atomic standard does not guarantee that the implementation will be lock-free on every platform, so it’s best to know the capabilities of your platform and toolchain. You can call <code>std::atomic&lt;&gt;::is_lock_free</code> to make sure.</p>

<p>Different CPU families support RMW in different ways. Processors such as PowerPC and ARM expose load-link/store-conditional instructions, which effectively allow you to implement your own RMW primitive at a low level, though this is not often done. The common RMW operations are usually sufficient.</p>

<p>As illustrated by the flowchart, atomic RMWs are a necessary part of lock-free programming even on single-processor systems. Without atomicity, a thread could be interrupted halfway through the transaction, possibly leading to an inconsistent state.</p>

<h2 id="toc_3">Compare-And-Swap Loops</h2>

<p>Perhaps the most often-discussed RMW operation is compare-and-swap (CAS). On Win32, CAS is provided via a family of intrinsics such as <code>_InterlockedCompareExchange</code>. Often, programmers perform compare-and-swap in a loop to repeatedly attempt a transaction. This pattern typically involves copying a shared variable to a local variable, performing some speculative work, and attempting to publish the changes using CAS:</p>

<pre><code class="language-c">void LockFreeQueue::push(Node* newHead)
{
    for (;;)
    {
        // Copy a shared variable (m_Head) to a local.
        Node* oldHead = m_Head;

        // Do some speculative work, not yet visible to other threads.
        newHead-&gt;next = oldHead;

        // Next, attempt to publish our changes to the shared variable.
        // If the shared variable hasn&#39;t changed, the CAS succeeds and we return.
        // Otherwise, repeat.
        if (_InterlockedCompareExchange(&amp;m_Head, newHead, oldHead) == oldHead)
            return;
    }
}
</code></pre>

<p>Such loops still qualify as lock-free, because if the test fails for one thread, it means it must have succeeded for another – though some architectures offer a weaker variant of CAS where that’s not necessarily true. Whenever implementing a CAS loop, special care must be taken to avoid the ABA problem.</p>

<h2 id="toc_4">Sequential Consistency</h2>

<p>Sequential consistency means that all threads agree on the order in which memory operations occurred, and that order is consistent with the order of operations in the program source code. Under sequential consistency, it’s impossible to experience memory reordering shenanigans like the one I demonstrated in a previous post.</p>

<p>A simple (but obviously impractical) way to achieve sequential consistency is to disable compiler optimizations and force all your threads to run on a single processor. A processor never sees its own memory effects out of order, even when threads are pre-empted and scheduled at arbitrary times.</p>

<p>Some programming languages offer sequentially consistency even for optimized code running in a multiprocessor environment. In C++11, you can declare all shared variables as C++11 atomic types with default memory ordering constraints. In Java, you can mark all shared variables as volatile. Here’s the example from my previous post, rewritten in C++11 style:</p>

<pre><code class="language-c++">std::atomic&lt;int&gt; X(0), Y(0);
int r1, r2;

void thread1()
{
    X.store(1);
    r1 = Y.load();
}

void thread2()
{
    Y.store(1);
    r2 = X.load();
}
</code></pre>

<p>Because the C++11 atomic types guarantee sequential consistency, the outcome r1 = r2 = 0 is impossible. To achieve this, the compiler outputs additional instructions behind the scenes – typically memory fences and/or RMW operations. Those additional instructions may make the implementation less efficient compared to one where the programmer has dealt with memory ordering directly.</p>

<h2 id="toc_5">Memory Ordering</h2>

<p>As the flowchart suggests, any time you do lock-free programming for multicore (or any symmetric multiprocessor), and your environment does not guarantee sequential consistency, you must consider how to prevent memory reordering.</p>

<p>On today’s architectures, the tools to enforce correct memory ordering generally fall into three categories, which prevent both compiler reordering and processor reordering:</p>

<ul>
<li>A lightweight sync or fence instruction, which I’ll talk about in future posts;</li>
<li>A full memory fence instruction, which I’ve demonstrated previously;</li>
<li>Memory operations which provide acquire or release semantics.</li>
</ul>

<p>Acquire semantics prevent memory reordering of operations which follow it in program order, and release semantics prevent memory reordering of operations preceding it. These semantics are particularly suitable in cases when there’s a producer/consumer relationship, where one thread publishes some information and the other reads it. I’ll also talk about this more in a future post.</p>

<h2 id="toc_6">Different Processors Have Different Memory Models</h2>

<p>Different CPU families have different habits when it comes to memory reordering. The rules are documented by each CPU vendor and followed strictly by the hardware. For instance, PowerPC and ARM processors can change the order of memory stores relative to the instructions themselves, but normally, the x86/64 family of processors from Intel and AMD do not. We say the former processors have a more relaxed memory model.</p>

<p>There’s a temptation to abstract away such platform-specific details, especially with C++11 offering us a standard way to write portable lock-free code. But currently, I think most lock-free programmers have at least some appreciation of platform differences. If there’s one key difference to remember, it’s that at the x86/64 instruction level, every load from memory comes with acquire semantics, and every store to memory provides release semantics – at least for non-SSE instructions and non-write-combined memory. As a result, it’s been common in the past to write lock-free code which works on x86/64, but fails on other processors.</p>

<p>If you’re interested in the hardware details of how and why processors perform memory reordering, I’d recommend Appendix C of Is Parallel Programming Hard. In any case, keep in mind that memory reordering can also occur due to compiler reordering of instructions.</p>

<p>In this post, I haven’t said much about the practical side of lock-free programming, such as: When do we do it? How much do we really need? I also haven’t mentioned the importance of validating your lock-free algorithms. Nonetheless, I hope for some readers, this introduction has provided a basic familiarity with lock-free concepts, so you can proceed into the additional reading without feeling too bewildered. As usual, if you spot any inaccuracies, let me know in the comments.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[this is my test blog]]></title>
    <link href="http://iliubang.github.io/14770594324221.html"/>
    <updated>2016-10-21T22:17:12+08:00</updated>
    <id>http://iliubang.github.io/14770594324221.html</id>
    <content type="html"><![CDATA[
<p><img src="media/14770594324221/php.jpeg" alt="php"/></p>

<pre><code class="language-php">class Demo 
{
    public function __construct()
    {
        echo &#39;hello world&#39;;
    }
}
</code></pre>

<h2 id="toc_0">test2</h2>

<p>\[ J_\alpha(x) = \sum_{m=0}^\infty \frac{(-1)^m}{m! \Gamma (m + \alpha + 1)} {\left({ \frac{x}{2} }\right)}^{2m + \alpha} \text {，独立公式示例} \]</p>

<p>\[\lambda\]</p>

]]></content>
  </entry>
  
</feed>
